#!/usr/bin/env perl
# Copyright (C) 2018  Alex Schroeder <alex@gnu.org>

# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program. If not, see <http://www.gnu.org/licenses/>.

use Modern::Perl;
use IO::Socket::IP;
use File::Copy qw(copy);
use URI;

# This is our directory. Every gopher URL we subscribe to will generate a file
# in this directory, eg. 'alexschroeder.ch:70'. We need this because we need to
# detect changes. We cannot rely on feeds.
our $data_dir = $ENV{MOKU_PONA};
$data_dir ||= $ENV{HOME} . '/.moku-pona' if $ENV{HOME};
$data_dir ||= $ENV{LOGDIR} . '/.moku-pona' if $ENV{LOGDIR};

# This is a gopher map listing all the pages being watched whih means that you
# can view using a gopher client. Or you can add it to your gopher site for
# others to see.
our $site_list = $data_dir . '/sites.txt';

# This is the generated list with all the updates.
our $updated_list = $data_dir . '/updates.txt';

sub query_gopher {
  my $selector = shift;
  my $host = shift;
  my $port = shift;

  # create client
  my $socket = IO::Socket::IP->new(
    PeerHost => $host,
    PeerPort => $port,
    Timeout  => 30,
    Type     => SOCK_STREAM, )
      or warn "Cannot construct client socket to $host:$port: $@\n";
  return unless $socket;
  my $result;
  eval {
    local $SIG{ALRM} = sub { die "timeout\n"; };
    alarm 10;
    $socket->print("$selector\r\n");
    sleep(1);
    shutdown($socket, 1);
    local $/ = undef; # slurp
    $result = <$socket>;
    alarm 0;
  };
  if ($@) {
    warn $@;
    return;
  }
  return $result;
}

sub load_site {
  my $file = shift;
  return [] if not -f $site_list;
  open(my $fh, "<:encoding(UTF-8)", $site_list)
      or die "Cannot read $site_list: $!\n";
  local $/ = "\r\n";
  # ignore info lines
  my @lines = grep(!/^i/, <$fh>);
  chomp(@lines);
  return \@lines;
}

sub load_file {
  my $file = shift;
  return "" if not -f $file;
  open(my $fh, "<:encoding(UTF-8)", $file)
      or die "Cannot read $file: $!\n";
  local $/ = undef; # slurp
  return <$fh>;
}

sub save_file {
  my $file = shift;
  my $data = shift;
  mkdir $data_dir unless -d $data_dir;
  open(my $fh, ">:encoding(UTF-8)", $file)
      or die "Cannot write $file: $!\n";
  print $fh $data;
}

sub url_to_gopher {
  my $str = shift;
  $str = "gopher://" . $str unless $str =~ /:\/\//;
  my $uri = URI->new($str);
  return if not $uri->scheme or $uri->scheme !~ /^gophers?$/;
  my $name = shift||$uri;
  my $path = $uri->path_query;
  $path = substr($path, 1) if substr($path, 0, 1) eq "/";
  my $type = $path ? substr($path, 0, 1) : "1";
  my $selector = $path ? substr($path, 1) : "";
  my $line = join("\t", $type . $name, $selector,
		  $uri->host, $uri->port||70);
  return $line;
}

sub do_add {
  my $uri = shift;
  my $name = shift;
  my $line = url_to_gopher($uri, $name);
  if (not $line) {
    warn("Only gopher URLs allowed\n");
    return;
  }
  my $site = load_site();
  if (not grep(/^$line$/, @$site)) {
    push(@$site, $line);
  } else {
    warn("$uri already exists in $site_list\n");
  }
  save_file($site_list, join("\r\n", @$site, ""));
}

sub do_remove {
  my @args = @_;
  my $site = load_site();
  my $count = 0;
  my $i = 0;
  while (@args and $i < @$site) {
    my $line = $site->[$i];
    my ($desc, $selector, $host, $port) = split(/\t/, substr($line, 1));
    my $found = 0;
    my $j = 0;
    while ($j < @args) {
      if ($desc eq $args[$j]) {
	$count++;
	$found = 1;
	splice(@$site, $i, 1); # remove the site found
	splice(@args, $j, 1); # remove the arg found
      } else {
	$j++;
      }
    }
    $i++ unless $found;
  }
  if ($count) {
    printf("Removed %d %s\n", $count,
	   $count == 1 ? "subscription" : "subscriptions");
    save_file($site_list, join("\r\n", @$site, ""));
  } else {
    warn("No subscriptions matching @args found\n");
    warn("Use moku-pona list to find the correct descriptions.\n");
  }
}

sub do_cleanup {
  my $confirm = shift||'' eq '--confirm';
  my $todo = 0;
  # get a hash map telling us the cache files we expect based on our sites
  my $site = load_site();
  my %caches = map {
    my ($desc, $selector, $host, $port) = split(/\t/, substr($_, 1));
    $port += 0;
    $selector =~ s/\//-/g;
    "$data_dir/$host-$port-$selector.txt" => 1;
  } @$site;
  # get a list of text files in the directory
  opendir(my $dh, $data_dir)
      or die "Cannot read $data_dir: $!\n";
  my @files = map { "$data_dir/$_" }
      grep { /\.txt$/ && -f "$data_dir/$_" }
      readdir($dh);
  closedir($dh);
  # remove unnecessary cache files
  for my $file (@files) {
    next if $file eq $site_list;
    next if $file eq $updated_list;
    next if $caches{$file};
    if ($confirm) {
      unlink $file;
    } else {
      print "trash $file\n";
      $todo++;
    }
  }
  # check updates list
  if (-f $updated_list) {
    open(my $fh, "<:encoding(UTF-8)", $updated_list)
	or die "Cannot read $updated_list: $!\n";
    local $/ = "\r\n";
    my @lines = <$fh>; # includes info items
    chomp(@lines);
    # decide what to do about each line in updates, ignoring the date
    my %sites = map { $_ => 1 } @$site;
    my @deletes;
    my @keeps;
    for my $line (@lines) {
      if ($line =~ /^1\d\d\d\d-\d\d-\d\d (.+)/ and not $sites{"1$1"}) {
	push(@deletes, $line);
	$todo++;
      } else {
	push(@keeps, $line);
      }
    }
    print "Removing these entries from updates:\n"
	. join("\n", @deletes, "") if @deletes and not $confirm;
    # save
    save_file($updated_list, join("\r\n", @keeps, "")) if $confirm;
  }

  if ($todo && !$confirm) {
    print "\n";
    print "Use moku-pona cleanup --confirm to do it.\n";
  }
}

sub convert {
  my @lines = @_;
  my $date;
  my @result;
  for (my $i = 0; $i < @lines; $i++) {
    if ($lines[$i] =~ /^i(\d\d\d\d-\d\d-\d\d)\t/) {
      $date = $1;
    } elsif ($date and $lines[$i] =~ /^1(.*)/) {
      push(@result, "1$date $1");
    }
  }
  return @result;
}

sub add_update {
  my $line = shift;
  # add current date
  my ($sec, $min, $hour, $mday, $mon, $year) = gmtime(); # UTC
  my $date = sprintf('%4d-%02d-%02d', $year + 1900, $mon + 1, $mday);
  $line = substr($line, 0, 1) . $date . " ". substr($line, 1);
  # load file
  my @lines;
  if (-f $updated_list) {
    open(my $fh, "<:encoding(UTF-8)", $updated_list)
	or die "Cannot read $updated_list: $!\n";
    local $/ = "\r\n";
    @lines = <$fh>;
    chomp(@lines);
  }
  # handle legacy format
  if (grep(/^i\d\d\d\d-\d\d-\d\d\t/, @lines)) {
    @lines = convert(@lines);
  }
  my $found = 0;
  # add it to the front but skip info lines
  for ($found = 0; $found < @lines; $found++) {
    last if $lines[$found] !~ /^i/;
  }
  splice(@lines, $found, 0, $line);
  # and remove any past mentions
  my $re = substr($line, 0, 1) . '\d\d\d\d-\d\d-\d\d' . substr($line, 11);
  for (my $i = $found + 1; $i < @lines; $i++) {
    if ($lines[$i] =~ /^$re/) {
      splice(@lines, $i, 1);
    }
  }
  # save
  save_file($updated_list, join("\r\n", @lines, ""));
}

# Convert a RSS or Atom feed to a Gopher map
sub to_gopher {
  my $xml = shift;
  require XML::LibXML;
  my $dom = XML::LibXML->load_xml(string => $xml);
  my $root = $dom->documentElement();
  my $xpc = XML::LibXML::XPathContext->new;
  $xpc->registerNs('atom', 'http://www.w3.org/2005/Atom');
  my $nodes = $xpc->findnodes('//atom:entry', $root) || $root->findnodes('//item');
  my @lines;
  for my $node ($nodes->get_nodelist) {
    my $titles = $xpc->findnodes('atom:title', $node) || $node->getChildrenByTagName('title');
    my $title = $titles->shift->textContent; # take the first
    my $links = $xpc->findnodes('atom:link', $node) || $node->getChildrenByTagName('link');
    my $link = $links->shift; # take the first
    my $href = $link->getAttribute('href') || $link->textContent;
    push(@lines, url_to_gopher($href, $title));
  }
  return join("\n", @lines, "");
}

sub do_update {
  my $quiet = shift||'' eq '--quiet';
  my $site = load_site();
  local $| = 1; # flush
  for my $line (@$site) {
    # skip item type
    my ($desc, $selector, $host, $port) = split(/\t/, substr($line, 1));
    $port += 0;
    print("Fetching $desc...") unless $quiet;
    my $new = query_gopher($selector, $host, $port);
    if (not $new) {
      warn("$desc returned an empty document\n");
      next;
    }
    $selector =~ s/\//-/g;
    my $cache = "$data_dir/$host-$port-$selector.txt";
    if ($new =~ /^<\?xml/) {
      $new = to_gopher($new);
      $line = join("\t", '1' . $desc, $cache, "", "");
    }
    my $old = load_file($cache);
    if ($new ne $old) {
      print("updated\n") unless $quiet;
      add_update($line);
      save_file($cache, $new);
    } else {
      print("unchanged\n") unless $quiet;
    }
  }
}

sub do_publish {
  my $target = shift;
  die "Target $target is not a directory\n" unless -d $target;
  die "Source $site_list does not exist\n" unless -f $site_list;
  die "Source $updated_list does not exist\n" unless -f $updated_list;
  my $path;
  # copy site list
  copy($site_list, "$target/sites.txt");
  # copy updates but with local links for the feed files
  open(my $in, "<:encoding(UTF-8)", $updated_list)
      or die "Cannot read $updated_list: $!\n";
  open(my $out, ">:encoding(UTF-8)", "$target/updates.txt")
      or die "Cannot write $target/updates.txt: $!\n";
  local $/ = "\r\n";
  for my $line (<$in>) {
    chomp($line);
    my @fields = split(/\t/, $line);
    my $from = $fields[1];
    my $to = $fields[1];
    # if the target is a local file, then that's because it is the result of a
    # to_gopher call in do_update, so we need to copy it as well, and we need to
    # change the line in updates.txt so that it no longer points to our
    # (private) data directory
    if ($to =~ s/^$data_dir\///) {
      copy($from, "$target/$to");
      $fields[1] = $to;
    }
    print $out join("\t", @fields) . $/;
  }
}

sub do_list {
  my $site = load_site();
  print("Subscribed items in $site_list:\n");
  print("none\n") unless @$site;
  for my $line (@$site) {
    # skip item type
    my ($desc, $selector, $host, $port) = split(/\t/, substr($line, 1));
    if ($selector) {
      print(qq{moku-pona add $host:$port/1$selector "$desc"\n});
    } else {
      print(qq{moku-pona add $host:$port "$desc"\n});
    }
  }
}

sub do_help {
  print <<"EOF";
moku-pona add gopher-url [description]

  This adds a gopher URL to the list of subscribed items. These are stored in
  $site_list. You can provide an optional description for this URL. If you don't
  provide a description, the URL will be used as the item's description.

  Example: moku-pona add alexschroeder.ch kensanata

moku-pona remove description

  This removes one or more gopher URL from the list of subscribed items. These
  are stored in $site_list.

  Example: moku-pona remove kensanata

moku-pona list

  This lists all the subscribed items.

moku-pona cleanup [--confirm]

  This deletes all the cached pages that we are no longer subscribed to. These
  are stored in $data_dir.

moku-pona update [--quiet]

  This updates all the subscribed items and generates a new local gopher menu
  for you to visit. This menu is available as $updated_list.

moku-pona publish <directory>

  This takes the important files from your data directory and copies them to a
  target directory. This is useful if you're publishing your feeds. You could
  just use symlinks but if you have an actuall *feed* in your subscriptions,
  that won't work as the cached file needs to be copied as well (and the links
  need to be changed, too).

EOF
}

sub main {
  my $command = shift(@ARGV) || "help";
  if ($command eq "add") { do_add(@ARGV) }
  elsif ($command eq "remove") { do_remove(@ARGV) }
  elsif ($command eq "list") { do_list() }
  elsif ($command eq "cleanup") { do_cleanup(@ARGV) }
  elsif ($command eq "update") { do_update(@ARGV) }
  elsif ($command eq "convert") { do_convert() }
  elsif ($command eq "publish") { do_publish(@ARGV) }
  else { do_help() }
}

main() if $0 =~ /\bmoku-pona$/;

1;
