#!/usr/bin/env perl
# Copyright (C) 2018–2020  Alex Schroeder <alex@gnu.org>

# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program. If not, see <http://www.gnu.org/licenses/>.

use Modern::Perl;
use IO::Socket::SSL;
use IO::Socket::IP;
use File::Copy qw(copy);
use URI;

# Optional (used when subscribing to feeds: XML::LibXML)

# This is our cache directory. Every Gopher or Gemini URL we subscribe to will
# generate a file in this directory, eg. 'gopher:--alexschroeder.ch:70' (the URL
# with all the slashes replaced by a hyphen). We need this because we need to
# detect changes. We cannot rely on feeds.
our $data_dir = $ENV{MOKU_PONA};
$data_dir ||= $ENV{HOME} . '/.moku-pona' if $ENV{HOME};
$data_dir ||= $ENV{LOGDIR} . '/.moku-pona' if $ENV{LOGDIR};

# This is a gopher map listing all the pages being watched which means that you
# can view using a gopher client. Or you can add it to your gopher site for
# others to see.
our $site_list = $data_dir . '/sites.txt';

# This is the generated list with all the updates.
our $updated_list = $data_dir . '/updates.txt';

sub query_gemini {
  my $url = shift;
  my($scheme, $authority, $path, $query, $fragment) =
      $url =~ m|(?:([^:/?#]+):)?(?://([^/?#]*))?([^?#]*)(?:\?([^#]*))?(?:#(.*))?|;
  die "⚠ The URL '$url' must use the gemini scheme\n" unless $scheme and $scheme eq 'gemini';
  die "⚠ The URL '$url' must have an authority\n" unless $authority;
  my ($host, $port) = split(/:/, $authority, 2);
  $port //= 1965;
  my $socket = IO::Socket::SSL->new(
    PeerHost => $host,
    PeerService => $port,
    SSL_verify_mode => SSL_VERIFY_NONE)
      or die "Cannot construct client socket: $@";
  # send data in one go
  print $socket "$url\r\n";
  # read response
  undef $/;
  my ($header, $response) = split(/\r\n/, <$socket>, 2);
  return $response;
}

sub query_gopher {
  my $selector = shift;
  my $host = shift;
  my $port = shift;
  # create client
  my $socket = IO::Socket::IP->new(
    PeerHost => $host,
    PeerPort => $port,
    Timeout  => 30,
    Type     => SOCK_STREAM, )
      or warn "Cannot construct client socket to $host:$port: $@\n";
  return unless $socket;
  my $result;
  eval {
    local $SIG{ALRM} = sub { die "timeout\n"; };
    alarm 10;
    $socket->print("$selector\r\n");
    sleep(1);
    shutdown($socket, 1);
    local $/ = undef; # slurp
    $result = <$socket>;
    alarm 0;
  };
  if ($@) {
    warn $@;
    return;
  }
  return $result;
}

sub url_to_gopher {
  my $str = shift;
  $str = "gopher://" . $str unless $str =~ /:\/\//;
  my $uri = URI->new($str);
  return if not $uri->scheme or $uri->scheme !~ /^gophers?$/;
  my $name = shift||$uri;
  my $path = $uri->path_query;
  $path = substr($path, 1) if substr($path, 0, 1) eq "/";
  my $type = $path ? substr($path, 0, 1) : "1";
  my $selector = $path ? substr($path, 1) : "";
  return ($selector, $uri->host, $uri->port||70);
}

sub convert {
  for (@_) {
    next if /^=> /; # is already a gemini link
    my ($type, $desc, $selector, $host, $port) = /^([^\t])([^\t]*)\t([^\t]*)\t([^\t]*)\t([^\t\r]*)/;
    if ($host and $port) {
      $port //= 0;
      $_ = "=> gopher://$host:$port/$type$selector $desc";
    }
  }
  return @_;
}

sub load_site {
  my $file = $site_list;
  return [] if not -f $file;
  open(my $fh, "<:encoding(UTF-8)", $file) or die "Cannot read $file: $!\n";
  my @lines = <$fh>;
  chomp(@lines);
  @lines = grep(/^=> /, convert(@lines)); # from gopher
  return \@lines;
}

sub load_file {
  my $file = shift;
  return "" if not -f $file;
  open(my $fh, "<:encoding(UTF-8)", $file)
      or die "Cannot read $file: $!\n";
  local $/ = undef;
  return <$fh>;
}

sub save_file {
  my $file = shift;
  my $data = shift;
  mkdir $data_dir unless -d $data_dir;
  open(my $fh, ">:encoding(UTF-8)", $file)
      or die "Cannot write $file: $!\n";
  print $fh $data;
}

sub do_add {
  my $uri = shift;
  my $name = shift;
  $name ||= $uri;
  my $line = "=> $uri $name";
  my $site = load_site();
  if (not grep(/^=> $uri /, @$site)) {
    push(@$site, $line);
  } else {
    warn("$uri already exists in $site_list\n");
  }
  save_file($site_list, join("\n", @$site, ""));
}

sub do_remove {
  my @args = @_;
  my $site = load_site();
  my $count = 0;
  my $i = 0;
  while (@args and $i < @$site) {
    my $line = $site->[$i];
    my ($uri, $name) = $line =~ /^=> (\S+)\s+(.*)/;
    my $found = 0;
    my $j = 0;
    while ($j < @args) {
      if ($name eq $args[$j]) {
	$count++;
	$found = 1;
	splice(@$site, $i, 1); # remove the site found
	splice(@args, $j, 1); # remove the arg found
      } else {
	$j++;
      }
    }
    $i++ unless $found;
  }
  if ($count) {
    printf("Removed %d %s\n", $count,
	   $count == 1 ? "subscription" : "subscriptions");
    save_file($site_list, join("\n", @$site, ""));
  } else {
    warn("No subscriptions matching @args found\n");
    warn("Use moku-pona list to find the correct descriptions.\n");
  }
}

sub do_cleanup {
  my $confirm = shift||'' eq '--confirm';
  my $todo = 0;
  # get a hash map telling us the cache files we expect based on our sites
  my $site = load_site();
  my %caches = map {
    my ($uri, $name) = /^=> (\S+)\s+(.*)/;
    $uri =~ s/\//-/g;
    "$data_dir/$uri" => 1;
  } @$site;
  # get a list of text files in the directory
  opendir(my $dh, $data_dir) or die "Cannot read $data_dir: $!\n";
  my @files = map { "$data_dir/$_" } grep { /^[^.]/ } readdir($dh);
  closedir($dh);
  # remove unnecessary cache files
  for my $file (@files) {
    next if $file eq $site_list;
    next if $file eq $updated_list;
    next if $caches{$file};
    if ($confirm) {
      unlink $file;
    } else {
      print "trash $file\n";
      $todo++;
    }
  }
  # check updates list
  if (-f $updated_list) {
    open(my $fh, "<:encoding(UTF-8)", $updated_list)
	or die "Cannot read $updated_list: $!\n";
    my @lines = <$fh>;
    chomp(@lines);
    # decide what to do about each line in updates, ignoring the date
    my %sites = map { $_ => 1 } @$site;
    my @deletes;
    my @keeps;
    for my $line (@lines) {
      if ($line =~ /^=> (\S+) \d\d\d\d-\d\d-\d\d (.+)/ and not $sites{"=> $1 $2"}) {
	push(@deletes, $line);
	$todo++;
      } else {
	push(@keeps, $line);
      }
    }
    print "Removing these entries from updates:\n"
	. join("\n", @deletes, "") if @deletes and not $confirm;
    # save
    save_file($updated_list, join("\n", @keeps, "")) if $confirm;
  }
  if ($todo && !$confirm) {
    print "\n";
    print "Use moku-pona cleanup --confirm to do it.\n";
  }
}

sub add_update {
  my $line = shift;
  # add current date
  my ($sec, $min, $hour, $mday, $mon, $year) = gmtime(); # UTC
  my $date = sprintf('%4d-%02d-%02d', $year + 1900, $mon + 1, $mday);
  my ($uri, $name) = $line =~ /^=> (\S+)\s+(.*)/;
  $line = "=> $uri $date $name";
  # load file
  my @lines;
  if (-f $updated_list) {
    open(my $fh, "<:encoding(UTF-8)", $updated_list)
	or die "Cannot read $updated_list: $!\n";
    @lines = convert(<$fh>); # from gohper
    chomp(@lines);
  }
  my $found = 0;
  # add it to the front but skip non-links
  for ($found = 0; $found < @lines; $found++) {
    last if $lines[$found] =~ /^=>/;
  }
  splice(@lines, $found, 0, $line);
  # and remove any past mentions
  my $re = "=> " . quotemeta($uri) . ' \d\d\d\d-\d\d-\d\d ' . quotemeta($name);
  for (my $i = $found + 1; $i < @lines; $i++) {
    if ($lines[$i] =~ /^$re/) {
      splice(@lines, $i, 1);
    }
  }
  # save
  save_file($updated_list, join("\n", @lines, ""));
}

# Convert a RSS or Atom feed to Gemini
sub to_gemini {
  my $xml = shift;
  my $dom = eval {
    require XML::LibXML;
    my $parser = XML::LibXML->new(recover => 2); # no errors, no warnings
    $parser->load_xml(string => $xml);
  };
  if ($@) {
    warn "$@\n";
    return '';
  }
  my $root = $dom->documentElement();
  my $xpc = XML::LibXML::XPathContext->new;
  $xpc->registerNs('atom', 'http://www.w3.org/2005/Atom');
  my $nodes = $xpc->findnodes('//atom:entry', $root) || $root->findnodes('//item');
  my @lines;
  for my $node ($nodes->get_nodelist) {
    my $titles = $xpc->findnodes('atom:title', $node) || $node->getChildrenByTagName('title');
    my $title = $titles->shift->textContent; # take the first
    $title =~ s/\s+$//; # trim right
    $title =~ s/^\s+//; # trim left
    my $links = $xpc->findnodes('atom:link', $node) || $node->getChildrenByTagName('link');
    next unless $links;
    my $link = $links->shift; # take the first
    my $href = $link->getAttribute('href') || $link->textContent;
    push(@lines, "=> $href $title");
  }
  return join("\n", @lines, "");
}

sub do_update {
  my $quiet = shift||'' eq '--quiet';
  my $site = load_site();
  local $| = 1; # flush
  for my $line (@$site) {
    my ($uri, $name) = $line =~ /^=> (\S+)\s+(.*)/;
    print("Fetching $name...") unless $quiet;
    my $new;
    if ($uri =~ /^gopher/) {
      $new = query_gopher(url_to_gopher($uri));
    } elsif ($uri =~ /^gemini/) {
      $new = query_gemini($uri);
    } else {
      warn "Skipping $uri...\n";
      next;
    }
    if (not $new) {
      warn("$name returned an empty document\n");
      next;
    }
    $uri =~ s/\//-/g;
    my $cache = "$data_dir/$uri";
    if ($new =~ /^<\?xml/) {
      $new = to_gemini($new);
      $line = "=> file:///$uri $name"; # now referring to the local cache file
    }
    my $old = load_file($cache);
    if ($new ne $old) {
      print("updated\n") unless $quiet;
      add_update($line);
      save_file($cache, $new);
    } else {
      print("unchanged\n") unless $quiet;
    }
  }
}

sub do_publish {
  my $target = shift;
  die "Target $target is not a directory\n" unless -d $target;
  die "Source $site_list does not exist\n" unless -f $site_list;
  die "Source $updated_list does not exist\n" unless -f $updated_list;
  my $path;
  # copy site list
  copy($site_list, "$target/sites.txt");
  # copy updates but with local links for the feed files
  open(my $in, "<:encoding(UTF-8)", $updated_list)
      or die "Cannot read $updated_list: $!\n";
  open(my $out, ">:encoding(UTF-8)", "$target/updates.txt")
      or die "Cannot write $target/updates.txt: $!\n";
  for my $line (<$in>) {
    chomp($line);
    ($line) = convert($line);
    my ($uri, $name) = $line =~ /^=> file:\/\/\/(\S+)\s+(.*)/;
    # if the target is a local file, then that's because it is the result of a
    # to_gemini call in do_update, so we need to copy it as well
    $uri =~ s/\//-/g;
    if (-f "$data_dir/$uri") {
      copy("$data_dir/$uri", "$target/$uri");
    }
    print $out "$line\n";
  }
}

sub do_list {
  my $site = load_site();
  print("Subscribed items in $site_list:\n");
  print("none\n") unless @$site;
  for my $line (@$site) {
    # skip item type
    my ($uri, $name) = $line =~ /^=> (\S+)\s+(.*)/;
    print(qq{moku-pona add $uri "$name"\n});
  }
}

sub do_help {
  print <<"EOF";
moku-pona add url [description]

  This adds a Gemini or Gopher URL to the list of subscribed items. If the
  target is an Atom or RSS feed, then that's also supported. These are stored in
  $site_list. You can provide an optional description for this URL. If you don't
  provide a description, the URL will be used as the item's description.

  Example: moku-pona add gemini://alexschroeder.ch kensanata

moku-pona remove description

  This removes one or more URLs from the list of subscribed items. These
  are stored in $site_list.

  Example: moku-pona remove kensanata

moku-pona list

  This lists all the subscribed items.

moku-pona cleanup [--confirm]

  This deletes all the cached pages that we are no longer subscribed to. These
  are stored in $data_dir.

moku-pona update [--quiet]

  This updates all the subscribed items and generates a new local page for you
  to visit. This menu is available as $updated_list.

moku-pona publish <directory>

  This takes the important files from your data directory and copies them to a
  target directory. This is useful if you're publishing your feeds. You could
  just use symlinks but if you have an actuall *feed* in your subscriptions,
  that won't work as the cached file needs to be copied as well.

EOF
}

sub main {
  my $command = shift(@ARGV) || "help";
  if ($command eq "add") { do_add(@ARGV) }
  elsif ($command eq "remove") { do_remove(@ARGV) }
  elsif ($command eq "list") { do_list() }
  elsif ($command eq "cleanup") { do_cleanup(@ARGV) }
  elsif ($command eq "update") { do_update(@ARGV) }
  elsif ($command eq "convert") { do_convert() }
  elsif ($command eq "publish") { do_publish(@ARGV) }
  else { do_help() }
}

main() if $0 =~ /\bmoku-pona$/;

1;
